{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d126521c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate  \n",
    "\n",
    "# Embeddings / LLM imports (choose what you need)\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.llms.ollama import Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca86e2",
   "metadata": {},
   "source": [
    "#### Each import sets up the pieces needed to load the PDF, process it into a searchable form, and run a conversation through a chat interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8656317a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"sk-proj-***\"\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "llama_model = \"llama3.1:8b\"\n",
    "db_name = \"vector_db\"\n",
    "pdf_path = \"monopoly.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506aee0c",
   "metadata": {},
   "source": [
    "### This section defines configuration variables:\n",
    "\n",
    "* **OPENAI_API_KEY:** Your OpenAI API key (redacted here). It’s needed if you use OpenAI models.\n",
    "\n",
    "* **openai_model:** The name of the OpenAI model to use. *\"gpt-4o-mini\"* refers to GPT-4o (“Omni”) Mini, a smaller GPT model.\n",
    "\n",
    "* **llama_model:** The name of the local model for Ollama. *\"llama3.1:8b\"* likely refers to a Llama 3 model with 8 billion parameters, pulled into Ollama’s local service. Ollama allows you to run such open-source LLMs locally.\n",
    "\n",
    "* **db_name:** Name of the directory where the vector database will be saved (\"vector_db\").\n",
    "\n",
    "* **pdf_path:** The path to the PDF file that will be used as the knowledge source (\"monopoly.pdf\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5978a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574584a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDFLoader(file_path: str):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load()\n",
    "\n",
    "def TextSplitter(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ebd58",
   "metadata": {},
   "source": [
    "### These two functions prepare the PDF content for processing:\n",
    "\n",
    "* **PDFLoader:** Takes a file path (string), creates a PyPDFLoader for that PDF, and calls *loader.load()*. This returns a list of LangChain Document objects, where each document represents the text of part of the PDF (for example, one page per document by default).\n",
    "\n",
    "* **TextSplitter:** Takes a list of Document objects and splits them into smaller chunks. It uses *RecursiveCharacterTextSplitter* with a *chunk_size* of 800 characters and an *chunk_overlap* of 80. This means each chunk is ~800 characters long, and adjacent chunks overlap by 80 characters to preserve context between splits. The *split_documents* method returns a new list of smaller Document chunks. This ensures the text is in manageable pieces for embedding and retrieval.\n",
    "\n",
    "Together, these helpers load the PDF content and divide it into semantically meaningful pieces. This is crucial for building an effective embedding index and for improving answer quality (smaller chunks keep context local)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8928288",
   "metadata": {},
   "source": [
    "## Embedding Factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21528f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHuggingFaceEmbedding():\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def GetOpenAIEmbedding():\n",
    "    # If you want openai embeddings instead, instantiate here (requires OpenAI key)\n",
    "    return OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c4215",
   "metadata": {},
   "source": [
    "### This section defines two helper functions to create embedding generator objects:\n",
    "\n",
    "* **GetHuggingFaceEmbedding:** Instantiates *HuggingFaceEmbeddings* using the model *sentence-transformers/all-MiniLM-L6-v2*. This is a popular, lightweight model for encoding sentences into numeric vectors.\n",
    "\n",
    "* **GetOpenAIEmbedding:** Instantiates *OpenAIEmbeddings* using the OpenAI model *text-embedding-3-small*, requiring your API key. This uses OpenAI’s embedding service to turn text into vectors.\n",
    "\n",
    "You can choose which embedding source to use by calling the appropriate function. For example, using Hugging Face embeddings avoids API calls but runs locally; OpenAI embeddings may yield different embeddings but require the API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9014f16",
   "metadata": {},
   "source": [
    "## Vector Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetVectorstore(chunks, embeddings, persist_directory=db_name):\n",
    "    try:\n",
    "        if os.path.exists(persist_directory):\n",
    "            shutil.rmtree(persist_directory)\n",
    "    except PermissionError:\n",
    "        print(f\"Warning: Could not delete {persist_directory}. Using existing database.\")\n",
    "    \n",
    "    return Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2145cd",
   "metadata": {},
   "source": [
    "### This function creates (or recreates) the vector database (Chroma) from text chunks:\n",
    "\n",
    "* It first checks if a directory named *persist_directory* (default db_name, i.e. \"vector_db\") exists. If so, it attempts to delete that directory with *shutil.rmtree*, ensuring we start fresh. If permission is denied (perhaps the directory is in use), it prints a warning and proceeds with the existing data.\n",
    "\n",
    "* Then it calls *Chroma.from_documents(...)* with the list of chunks and the specified embeddings. This builds a Chroma vectorstore: each chunk of text is converted to a vector (using the embeddings provided) and stored in the database.\n",
    "\n",
    "* Because *persist_directory* is given, Chroma will save the index files there. The LangChain docs note that a *persist_directory* means the collection will be saved to disk. This allows the index to be reused without recomputing in future runs.\n",
    "\n",
    "*In summary*, GetVectorstore prepares a searchable semantic index of the PDF content. This index lets us later find relevant chunks when a user asks a question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16cd5f",
   "metadata": {},
   "source": [
    "## Loading PDF & Building the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = PDFLoader(pdf_path)\n",
    "chunks = TextSplitter(documents)\n",
    "\n",
    "def Embeddings(model): \n",
    "    if model == \"GPT4o-mini\":\n",
    "        return GetOpenAIEmbedding()\n",
    "    elif model == \"Llama3-8b\":\n",
    "        return GetHuggingFaceEmbedding()\n",
    "\n",
    "vectorstore = GetVectorstore(chunks, Embeddings(\"GPT4o-mini\"), persist_directory=db_name)\n",
    "#vectorstore = GetVectorstore(chunks, Embeddings(\"Llama3-8b\"), persist_directory=db_name)\n",
    "print(\"Vectorstore created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7363dc",
   "metadata": {},
   "source": [
    "### This part of the code actually runs the loading and vector store creation:\n",
    "\n",
    "**1. Load PDF**\n",
    "\n",
    "**2. Split text**\n",
    "\n",
    "**3. Select embeddings:** The Embeddings function is intended to pick an embedding factory based on the model name. if model is \"GPT4o-mini\", use OpenAI embeddings; if \"Llama3-8b\", use Hugging Face.\n",
    "\n",
    "**4. Build vectorstore:** *GetVectorstore(chunks, Embeddings(\"GPT4o-mini\"), persist_directory=db_name)* is called. Here *\"GPT4o-mini\"* is passed to *Embeddings*, but due to the code, it ends up using Hugging Face embeddings on the chunks. The resulting vectorstore is saved in *\"vector_db\"*.\n",
    "\n",
    "At this point, we have a Chroma database of vectors for each text chunk from the PDF. This database can be queried semantically to find the chunks most relevant to any question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebddb8",
   "metadata": {},
   "source": [
    "## Conversational Retrieval with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299aee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global memory object to persist conversation history across queries\n",
    "global_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "def ConversationRetrieval(query: str, llm, memory=global_memory):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    custom_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Use the following context to answer the question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        combine_docs_chain_kwargs={\"prompt\": custom_prompt}\n",
    "    )\n",
    "\n",
    "    result = conversation_chain.invoke({\"question\": query})\n",
    "    return result.get(\"answer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc13c8",
   "metadata": {},
   "source": [
    "### This section defines how to answer a query using the vectorstore and memory:\n",
    "\n",
    "* **Memory:** global_memory is a *ConversationBufferMemory* that stores all past messages. It has *return_messages=True*, meaning it keeps the history as a list of messages. This memory is passed to the chain so that each new question can be answered in context.\n",
    "\n",
    "* **ConversationRetrieval function:** Takes a user query (string) and an LLM instance.\n",
    "\n",
    "    * It creates a retriever from the vectorstore with *k=5*, meaning up to 5 relevant documents (text chunks) will be retrieved for each query.\n",
    "\n",
    "    * It defines a *custom_prompt* template. This template tells the model: *“Use the following context to answer the question...”* and fills in *{context} (retrieved docs)* and *{question}*.\n",
    "\n",
    "    * It builds a *ConversationalRetrievalChain* with the given *llm*, *retriever*, and *memory*. The *combine_docs_chain_kwargs* include our *custom_prompt* for how to present the retrieved docs to the model.\n",
    "\n",
    "    * Finally, it calls *conversation_chain.invoke({\"question\": query})*. This runs the chain: it uses the chat history and new question to fetch relevant documents and then generates an answer. It returns the generated answer string.\n",
    "\n",
    "**In summary**, this sets up a retrieval-augmented generation (RAG) system with chat memory. According to the LangChain docs, *“This chain takes in chat history ... and new questions, and then returns an answer”* by combining retrieval and LLM steps. The conversation history plus the newly retrieved context helps the model answer accurately and consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87db57e",
   "metadata": {},
   "source": [
    "## Query Wrapper with Persistent Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e427895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Query(query: str, model: str = \"Llama\"):\n",
    "    if model == \"Llama3-8b\":\n",
    "        llm = Ollama(model=llama_model)\n",
    "    elif model == \"GPT4o-mini\":\n",
    "        llm = ChatOpenAI(model_name=openai_model, api_key=OPENAI_API_KEY)\n",
    "\n",
    "    # Use the global memory object to persist history across calls\n",
    "    return ConversationRetrieval(query, llm, memory=global_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80021a8",
   "metadata": {},
   "source": [
    "This function chooses which LLM to use based on the model string and then calls ConversationRetrieval. calls *ConversationRetrieval(query, llm, memory=global_memory)* to get the answer. Because *global_memory* is passed each time, the chat history persists across multiple queries.\n",
    "\n",
    "**In effect**, Query hides the details of model selection and ensures that each new user question is processed with the **same memory** object, allowing the chatbot to carry context from previous turns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76394840",
   "metadata": {},
   "source": [
    "## Gradio Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Wrap for Gradio Chat ---\n",
    "def ChatBot(query, history, model):\n",
    "    answer = Query(query, model)  \n",
    "    return answer\n",
    "\n",
    "# --- Chat Interface ---\n",
    "with gr.Blocks() as ui:\n",
    "    gr.Markdown(\"## 📘 PDF Q&A Chatbot with Memory\")\n",
    "    gr.Markdown(\"#### Ask questions about the $Monopoly$ board game PDF using *Llama3-8b* or *GPT4o-mini*.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        model_selector = gr.Dropdown(\n",
    "            choices=[\"Llama3-8b\", \"GPT4o-mini\"],\n",
    "            label=\"Choose Model\",\n",
    "            value=\"GPT4o-mini\"\n",
    "        )\n",
    "\n",
    "    chatbot = gr.Chatbot(height=400)\n",
    "\n",
    "    msg = gr.Textbox(\n",
    "        placeholder=\"Type your question...\",\n",
    "        label=\"Your Question\"\n",
    "    )\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def user_submit(query, history, model):\n",
    "        # Append user query\n",
    "        history = history + [(query, None)]\n",
    "        answer = Query(query, model)\n",
    "        # Append bot answer\n",
    "        history[-1] = (query, answer)\n",
    "        return history, \"\"\n",
    "\n",
    "    msg.submit(user_submit, [msg, chatbot, model_selector], [chatbot, msg])\n",
    "\n",
    "ui.launch(inbrowser=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c829a39",
   "metadata": {},
   "source": [
    "**Overall**, this section uses Gradio’s Blocks API to create a simple chatbot UI. It combines static text (Markdown) and interactive elements (dropdown, textbox, chatbot panel). The logic ensures each user question is processed by the Query function (which uses the selected LLM and the memory-augmented retrieval chain), and the resulting answer is displayed in the chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59544821",
   "metadata": {},
   "source": [
    "### Example without interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ddc5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Example ----------\n",
    "if __name__ == \"__main__\":\n",
    "    answer = Query(\"How can I buy a hotel?\", \"Llama\")\n",
    "    print(\"Answer:\\n\", answer)\n",
    "     You can continue the conversation and the memory/history will be preserved:\n",
    "    followup = Query(\"What if I don't have enough money?\", \"Llama\")\n",
    "    print(\"Follow-up Answer:\\n\", followup)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
